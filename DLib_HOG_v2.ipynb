{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a function that detects the face, using the Dlib library methods.\n",
    "# In addition, let's combine some of the functionality with cv2, so we can render the rectangles on people's faces\n",
    "# that were detected.\n",
    "#Lastly, after detecing the face, crop out the said face and process it through the FER to determine an emotion\n",
    "def detectFaceDlib(detector, frame, data, inHeight=1650, inWidth=0):\n",
    "    frameDlibHog = frame.copy()\n",
    "    frameHeight = frameDlibHog.shape[0]\n",
    "    frameWidth = frameDlibHog.shape[1]\n",
    "    \n",
    "    if not inWidth:\n",
    "        inWidth = int((frameWidth / frameHeight) * inHeight)\n",
    "    \n",
    "    scaleHeight = frameHeight / inHeight\n",
    "    scaleWidth = frameWidth / inWidth\n",
    "    \n",
    "    frameDlibHogSmall = cv2.resize(frameDlibHog, (inWidth, inHeight))\n",
    "    faceRects = detector(frameDlibHogSmall, 0)\n",
    "    print(frameWidth, frameHeight, inWidth, inHeight)\n",
    "    bboxes = []\n",
    "    \n",
    "    for faceRect in faceRects:\n",
    "        cvRect = [int(faceRect.left() * scaleWidth), int(faceRect.top() * scaleHeight), int(faceRect.right() * scaleWidth), int(faceRect.bottom() * scaleHeight)]\n",
    "        bboxes.append(cvRect)\n",
    "        cv2.rectangle(frameDlibHog, (cvRect[0], cvRect[1]), (cvRect[2], cvRect[3]), (0, 255, 0), int(round(frameHeight/250)), 4)    \n",
    "    \n",
    "    for i, d in enumerate(faceRects):\n",
    "        crop = frameDlibHogSmall[d.top():d.bottom(), d.left():d.right()]\n",
    "        result = DeepFace.analyze(crop, actions = ['emotion'], enforce_detection =  False)\n",
    "        if(result['dominant_emotion'] == 'neutral'):\n",
    "            data[0] += 1\n",
    "        elif(result['dominant_emotion'] == 'angry'):\n",
    "            data[1] += 1\n",
    "        elif(result['dominant_emotion'] == 'disgust'):\n",
    "            data[2] += 1\n",
    "        elif(result['dominant_emotion'] == 'fear'):\n",
    "            data[3] += 1\n",
    "        elif(result['dominant_emotion'] == 'happy'):\n",
    "            data[4] += 1\n",
    "        elif(result['dominant_emotion'] == 'sad'):\n",
    "            data[5] += 1\n",
    "        elif(result['dominant_emotion'] == 'surprise'):\n",
    "            data[6] += 1\n",
    "            \n",
    "    return frameDlibHog, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want very high accuracy on facial recognition, keep the input height above 700.\n",
    "# However, the only downside of having high accuracy for facial recognition is low framerate.\n",
    "# If we want low accuracy but high framerate, then lower the input height below 500.\n",
    "# Expect the software to have issues when analyzing faces that are below 100 or so pixels.\n",
    "# It is typically accurate when the pixels are NOT below 100 pixels; otherwise, expect small frames to not be captured.\n",
    "\n",
    "# In addition, the individual needs to look at the camera for the software to recognize their face.\n",
    "# Otherwise, if they are NOT looking at the camera, their face will not be read.\n",
    "\n",
    "# Take aways\n",
    "# 1. higher facial detection accuracy = slower run-time.\n",
    "# 2. lower facial detection accuracy = faster run-time.\n",
    "# 3. If the lighting is bad, the face will not be read.\n",
    "# 4. If the frame is simply too small (less than 100 pixels), then expect the face to not be read.\n",
    "# 5. HOG outperforms MMOD in every way, including accuracy. It also doesn't need a .dat library.\n",
    "# 6. 1200 for input height seems to be the sweet spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = [0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    hogFaceDetector = dlib.get_frontal_face_detector()\n",
    "    \n",
    "    source = '00002.mp4' # File path a video here, it can be only accept one file as it should.\n",
    "    \n",
    "    cap = cv2.VideoCapture(source)\n",
    "    hasFrame, frame = cap.read()\n",
    "    \n",
    "    vid_writer = cv2.VideoWriter('output-hog-{}.avi'.format(str(source).split(\".\")[0]), cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 15, (frame.shape[1], frame.shape[0])) #Output the results onto a video.\n",
    "    \n",
    "    frame_count = 0 # We need a frame counter.\n",
    "    tt_dlibHog = 0\n",
    "    \n",
    "    while(1):\n",
    "        hasFrame, frame = cap.read()\n",
    "        \n",
    "        if not hasFrame:\n",
    "            break\n",
    "            \n",
    "        frame_count += 1\n",
    "        \n",
    "        t = time.time()\n",
    "        \n",
    "        outDlibHog, bboxes = detectFaceDlib(hogFaceDetector, frame, data)\n",
    "                \n",
    "        tt_dlibHog += time.time()  - t\n",
    "        fpsDlibHog = frame_count / tt_dlibHog\n",
    "        \n",
    "        label = \"DLIB HoG ; FPS : {:.2f}\".format(fpsDlibHog)\n",
    "        cv2.putText(outDlibHog, label, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "    \n",
    "        \n",
    "        cv2.imshow(\"Face Detection Comparison\", outDlibHog)\n",
    "\n",
    "        vid_writer.write(outDlibHog)\n",
    "        if frame_count == 1:\n",
    "            tt_dlibHog = 0\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            emotions = ('neutral', 'angry', 'disgust', 'fear', 'happy', 'sad', 'surprise')\n",
    "            y_pos = np.arange(len(emotions))\n",
    "            plt.bar(y_pos, data, align ='center', alpha = .5)\n",
    "            plt.xticks(y_pos, emotions)\n",
    "            plt.ylabel('Occurances')\n",
    "            plt.title('Number of Emotion Occurances in *[VIDEO NAME]*')\n",
    "            plt.show()\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    vid_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c882a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
